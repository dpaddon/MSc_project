#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jul 10 09:54:47 2018

@author: daniel

File to create a comprehensive data set drawing from the original
images, annotations generated by the tierpsy tracker, and 
hand-drawn annotations
"""

import os
import cv2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from copy import copy
import tables

from utility_functions import masks_from_XML


# define the various directories
DATA_DIR = '/Users/daniel/Documents/UCL/Project/Data/annotation-data'
VIDEOS_DIR = os.path.join(DATA_DIR, 'MaskedVideos')
FEATURES_DIR = os.path.join(DATA_DIR, 'Results')
ANNS_DIR = os.path.join(DATA_DIR, 'annotations_final')
#OUTPUT_DIR = os.path.join(DATA_DIR, 'collated_dataset')



###############################################################
# Define the frames to ignore due to poor quality hand annotations

exclude_file_names = ["CB4856_worms10_food1-10_Set1_Pos4_Ch2_20102017_125044", 
              "JU2234_worms10_food1-10_Set1_Pos4_Ch3_20102017_125033", 
              "JU2578_worms10_food1-10_Set1_Pos4_Ch4_20102017_125033", 
              "N2_worms10_food1-10_Set1_Pos4_Ch5_20102017_125024", 
              "VC2010_worms10_food1-10_Set1_Pos4_Ch6_20122017_150107", 
              "CX11271_worms10_food1-10_Set1_Pos4_Ch4_19052017_113042", 
              "ED3049_worms10_food1-10_Set6_Pos5_Ch4_19052017_151021",
              "JU360_worms10_food1-10_Set6_Pos5_Ch6_19052017_151012"]

exclude_lists = [[561, 2871, 2941, 3501, 4411, 4551, 5251, 13441, 14001, 16451, 21771, 21911, 22191, 22331],
                [1191, 1331, 1611, 3431, 4061, 6021, 6411, 7071, 12321, 2031, 10851, 11061],
                [141, 211, 421, 1331, 7981, 8611, 12601, 14001, 14071, 15681, 18551, 981, 1331, 1891, 4481, 13231, 9451, 10711, 15821],
                [8621, 13371, 20231],
                [9381, 11411, 12181, 14211, 22261, 11411, 21351, 11551, 16521],
                [1681, 9451, 19251, 20371, 1821, 4691, 4761, 4831, 4901, 4971, 5041, 1961, 2731, 2801, 2871, 2941, 2871],
                [10781,11761, 13441, 15961, 16031, 17571, 17991, 19111, 19181, 20791, 21141, 18271],
                [1471, 17361, 9311, 9381, 10361, 12321, 17641, 21281, 22471]]

exclude_dict = {}
for (fn, li) in zip(exclude_file_names, exclude_lists):
    exclude_dict[fn] = sorted(list(set(li)), key=int)
###############################################################



# get list of file names
# fNames = sorted([f for f in os.listdir(ANNS_DIR) if not f.startswith('.') if not f.endswith('212337')]) #exclude the worst annotation folder
fNames = sorted(exclude_file_names)
print("Filenames: ")
print("\n".join(fNames))
print("")


# Loop through each of the datasets
for filename in fNames:

    # set which dataset we are using
    print("Set being created: {}".format(filename))
    
    # define the filenames for the images, features, and XML annotations
    images_file = os.path.join(VIDEOS_DIR, filename + ".hdf5")
    features_file = os.path.join(FEATURES_DIR, filename + "_featuresN.hdf5")
    XML_DIR = os.path.join(ANNS_DIR, filename)

    CROPPED_OUTPUT_DIR = os.path.join(DATA_DIR, 'cropped_collated_dataset', filename)
    FULLSIZE_OUTPUT_DIR = os.path.join(DATA_DIR, 'fullsize_collated_dataset', filename)

    # load images file
    with pd.HDFStore(features_file, 'r') as fid:
        #all the worm coordinates and how the skeletons matrix related with a given frame is here
        trajectories_data = fid['/trajectories_data'] 
        
    # Get total number of frames in file:
    num_frames = trajectories_data['frame_number'].max()
    
    # Load every 70th frame
    for frame_number in range(num_frames)[1::70]:
        
        #############
        # Ignore frames in the exlcude list
        if frame_number in exclude_dict[filename]:
            continue
        #############
        
        print(frame_number)

        # read image (full or masked)
        img_field = '/mask'
#        img_field = "/full_data"

        # Select only the data for this frame
        traj_g = trajectories_data.groupby('frame_number')
        frame_data = traj_g.get_group(frame_number)
    
        # load existing annotations
        
        # Select only skeletonised worms
        # worms that where not succesfully skeletonized will have a -1 here
        skel_id = frame_data['skeleton_id'].values
        neg_skel_id = skel_id[skel_id<0]
        skel_id = skel_id[skel_id>=0]
        
        # Open the frame from the hdf5 file
        with tables.File(images_file, 'r') as fid:
            img = fid.get_node(img_field)[frame_number]
            img = img.T
        
        # get the worm contour coordinates
        with tables.File(features_file, 'r') as fid:
            # Reduce the coordinates by a factor of 10 so that they
            # match the image dimensions
            skel = fid.get_node('/coordinates/skeletons')[skel_id, :, :]/10
            cnt1 = fid.get_node('/coordinates/dorsal_contours')[skel_id, :, :]/10
            cnt2 = fid.get_node('/coordinates/ventral_contours')[skel_id, :, :]/10
        
#        # Plot the image with Tierpsy annotations
#        # Note we have to transpose the image to match the XML annotations
#        # Suspect this is due to MATLAB vs Numpy / col- vs row-major indexing
        
#        plt.figure(figsize=(30,30))
#        plt.imshow(img.T, interpolation='none', cmap='gray')
#        
#        #add all the worms identified
#        for _, row in frame_data.iterrows():
#           cc = plt.Circle((row['coord_y'], row['coord_x']), \
#                           row['roi_size']/2, lw=2, color='g', fill=False)
#           plt.gca().add_artist(cc)
#        
#        # add all the skeletonized worms
#        
#        # We also have to transpose the X and Y coordinates of the plots
#        for (ss, cc1, cc2) in zip(skel, cnt1, cnt2):
#            plt.plot(ss[:, 1], ss[:, 0], 'r')
#            plt.plot(cc1[:, 1], cc1[:, 0], 'tomato')
#            plt.plot(cc2[:, 1], cc2[:, 0], color='salmon')
#            
#        plt.show()
#        plt.close()

        # Get annotations as complete masks, starting with existing data
        masks = []
        
        # Loop through all of the existing worms 
        for (cc1, cc2) in zip(cnt1, cnt2):
            cnt_close = np.vstack([cc1, cc2[-1::-1]])
            
            # convert the outline to a solid mask
            mask = np.zeros(img.shape)
            cv2.fillPoly(mask, pts =[np.int32(cnt_close)], color=(255,255,255))
            
            #append this mask to our list of masks
            # note we have to transpose this mask (matlab vs numpy matrix indexing)
            masks.append(copy(mask.T))
            
                
        # load xml annotations (if they exist for this frame)
        # and append them to the list of masks
        annotation_path = os.path.join(XML_DIR, str(frame_number) + ".xml")
        if os.path.exists(annotation_path):
            xml_masks, xml_heads = masks_from_XML(annotation_path, img)
            masks.extend(copy(xml_masks))
            
 
        # Save fullsize images and masks
        os.makedirs(FULLSIZE_OUTPUT_DIR + '/{}/image'.format(frame_number), exist_ok=True)
        image_filename = FULLSIZE_OUTPUT_DIR + '/{}/image/image_{}.png'.format(frame_number,frame_number)
        plt.imsave(fname=image_filename, arr=img, format='png', cmap='gray')
        
        j = 0
        for m in masks:
            os.makedirs(FULLSIZE_OUTPUT_DIR + '/{}/masks'.format(frame_number), exist_ok=True)
            mask_filename = FULLSIZE_OUTPUT_DIR + '/{}/masks/mask_{}.png'.format(frame_number,j)
            plt.imsave(fname=mask_filename, arr=m, format='png', cmap='gray')
            j += 1
    
    
        # Splitting the images into 16 smaller chunks
        # Originially the images are 2048*2048 however this is far too large
        # to fit in to GPU RAM. 

        h = img.shape[0]
        w = img.shape[1]
        
        for x in range(4):
            for y in range(4):
                
                # We loop through all the chunks and set pos_example = True if
                # the chunk contains at least part of a worm, keeping only
                # those chunks.
                pos_example = False
        
        
                j = 0
                
                # Loop through all of the worms 
                for m in masks: 
                    # crop the mask for the chunk being examined
                    cropped_mask = m[int((h/4)*x):int((h/4)*(x+1)), int((w/4)*y):int((w/4)*(y+1))]
        
                    # if the chunk contains a worm:
                    if np.any(cropped_mask):
                        # Create a subdir for the masks for this crop
                        os.makedirs(CROPPED_OUTPUT_DIR + '/{}_{}{}/masks'.format(frame_number,x,y), exist_ok=True)
                        mask_filename = CROPPED_OUTPUT_DIR + '/{}_{}{}/masks/mask_{}.png'.format(frame_number,x,y,j)
                        plt.imsave(fname=mask_filename, arr=cropped_mask, format='png', cmap='gray')
                        
                        # Set this flag True to save this image crop
                        pos_example = True
        
                    j += 1
        
                if pos_example:
                     os.makedirs(CROPPED_OUTPUT_DIR + '/{}_{}{}/image'.format(frame_number,x,y), exist_ok=True)
                     cropped_img = img[int((h/4)*x):int((h/4)*(x+1)), int((w/4)*y):int((w/4)*(y+1))]
                     image_filename = CROPPED_OUTPUT_DIR + '/{}_{}{}/image/image_{}_{}{}.png'.format(frame_number,x,y,frame_number,x,y)
                     plt.imsave(fname=image_filename, arr=cropped_img, format='png', cmap='gray')
        

        
#TODO: make this script output tf.record files (sharded by set?)
        
        
        
